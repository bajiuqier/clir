import argparse
from pathlib import Path
from transformers import SchedulerType

HOME_DIR = Path.home() / 'Desktop'

def parse_args():
    parser = argparse.ArgumentParser(description="myself argments")
    parser.add_argument(
        "--log_path",
        type=str,
        default=str(HOME_DIR / 'clir' / 'training_logs'),
        help="è®­ç»ƒæ—¥å¿—æ–‡ä»¶è·¯å¾„"
    )
    # ------------------------------------- train/val/test data_file -------------------------------------
    parser.add_argument(
        "--train_file",
        type=str,
        default=str(HOME_DIR / 'clir' / 'data_processing' / 'data_file' / 'train_data.jsonl'),
        help="A csv or a json file containing the training data."
    )
    parser.add_argument(
        "--validation_file",
        type=str,
        default=None,
        help="A csv or a json file containing the validation data."
    )
    parser.add_argument(
        "--test_file", type=str, default=None, help="A csv or a json file containing the test data."
    )

    # ------------------------------------- tokenizer -------------------------------------
    parser.add_argument(
        "--max_length",
        type=int,
        default=128,
        help=(
            "The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,"
            " sequences shorter will be padded if `--pad_to_max_length` is passed."
        ),
    )
    parser.add_argument(
        "--pad_to_max_length",
        type=bool,
        default=True,
        help="If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.",
    )
    parser.add_argument(
        "--tokenizer_name",
        type=str,
        default=None,
        help="åŠ è½½ tokenizer_name ",
    )
    parser.add_argument(
        "--use_slow_tokenizer",
        action="store_true",
        help="If passed, will use a slow tokenizer (not backed by the ğŸ¤— Tokenizers library).",
    )
    # ------------------------------------- model -------------------------------------
    parser.add_argument(
        "--config_name",
        type=str,
        default=None,
        help="åŠ è½½ æ¨¡å‹çš„configé…ç½®",
    )
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        default=str(HOME_DIR / 'clir' / 'models' / 'models--xlm-roberta-base'),
        help="Path to pretrained model or model identifier from huggingface.co/models.",
        # required=True,
    )
    parser.add_argument(
        "--pooling_method",
        type=str,
        default='cls',
        help="pooling_method cls or mean",
    )
    parser.add_argument(
        "--normlized",
        type=bool,
        default=True,
        help="å¯¹æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬å‘é‡ è¿›è¡Œ normlized æ“ä½œ",
    )
    parser.add_argument(
        "--similarity_method",
        type=str,
        default='l2',
        help="ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼ cosã€l2",
    )

    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=32,
        help="Batch size (per device) for the training dataloader.",
    )
    parser.add_argument(
        "--per_device_eval_batch_size",
        type=int,
        default=8,
        help="Batch size (per device) for the evaluation dataloader.",
    )
    parser.add_argument(
        "--per_device_test_batch_size",
        type=int,
        default=32,
        help="Batch size (per device) for the evaluation dataloader.",
    )
    # ------------------------------------- å­¦ä¹ ç‡/ä¼˜åŒ–å™¨ç›¸å…³å‚æ•° -------------------------------------
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument("--weight_decay", type=float, default=0.001, help="Weight decay to use.")
    parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help=(
            "Number of updates steps to accumulate before performing a backward/update pass. åœ¨æ‰§è¡Œåå‘ä¼ æ’­/æ›´æ–°ä¼ é€’ä¹‹å‰ç´¯ç§¯çš„æ›´æ–°æ­¥éª¤æ•° "
            "å®ç°æ¢¯åº¦ç´¯ç§¯ å³åœ¨æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹å‰ç´¯ç§¯å¤šä¸ªå°æ‰¹æ¬¡ (mini-batch) çš„æ¢¯åº¦ã€‚"
            "è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹éå¸¸æœ‰æ„ä¹‰ï¼š"
                "å°æ˜¾å­˜è®¾å¤‡ä¸Šçš„å¤§æ‰¹æ¬¡è®­ç»ƒï¼š"
                "- è®­ç»ƒå¤§å‹æ¨¡å‹æˆ–ä½¿ç”¨å¤§æ‰¹æ¬¡æ—¶ï¼Œå•ä¸ª GPU çš„æ˜¾å­˜å¯èƒ½ä¸è¶³ä»¥å¤„ç†ä¸€æ¬¡å®Œæ•´çš„å¤§æ‰¹æ¬¡ã€‚"
                "- é€šè¿‡æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥åœ¨å¤šä¸ªå°æ‰¹æ¬¡ä¸Šè®¡ç®—æ¢¯åº¦ï¼Œå¹¶åœ¨ç´¯ç§¯åˆ°è¶³å¤Ÿå¤§çš„æ‰¹æ¬¡åå†æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»è€Œæ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒã€‚"
                "æé«˜ç¨³å®šæ€§å’Œæ³›åŒ–æ€§èƒ½ï¼š"
                "- ä½¿ç”¨è¾ƒå¤§çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒï¼Œé€šå¸¸å¯ä»¥ä½¿æ¢¯åº¦ä¼°è®¡æ›´åŠ ç¨³å®šï¼Œä»è€Œæœ‰åŠ©äºæ¨¡å‹çš„æ”¶æ•›å’Œæ³›åŒ–æ€§èƒ½ã€‚"
                "- é€šè¿‡æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ å®é™…æ˜¾å­˜éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œè·å¾—è¾ƒå¤§æ‰¹æ¬¡è®­ç»ƒçš„éƒ¨åˆ†å¥½å¤„ã€‚"
            "å‡è®¾ args.gradient_accumulation_steps = 4 å¹¶ä¸”æ¯ä¸ªå°æ‰¹æ¬¡çš„å¤§å°ä¸º 16ã€‚"
            "è¿™æ · é€šè¿‡æ¢¯åº¦ç´¯ç§¯ æ¨¡å‹å‚æ•°æ¯ç»è¿‡ 4 ä¸ªå°æ‰¹æ¬¡ (å³æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ä¸º 64) æ‰æ›´æ–°ä¸€æ¬¡ è¿™æ ·å¯ä»¥åœ¨ä¸å¢åŠ æ˜¾å­˜éœ€æ±‚çš„æƒ…å†µä¸‹ æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡çš„è®­ç»ƒ "
        )
    )
    parser.add_argument(
        "--lr_scheduler_type",
        type=SchedulerType,
        default="linear",
        help="The scheduler type to use.",
        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
    )
    parser.add_argument(
        "--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
    )

    # ------------------------------------- model train -------------------------------------
    parser.add_argument(
        "--output_dir",
        type=str,
        default=str(HOME_DIR / 'clir' / 'myself' / 'output'),
        help="Where to store the final model."
    )
    parser.add_argument("--seed", type=int, default=42, help="A seed for reproducible training.")
    parser.add_argument(
        "--trust_remote_code",
        type=bool,
        default=False,
        help=(
            "Whether or not to allow for custom models defined on the Hub in their own modeling files. This option "
            "should only be set to `True` for repositories you trust and in which you have read the code, as it will "
            "execute code present on the Hub on your local machine."
        ),
    )
    parser.add_argument(
        "--checkpointing_steps",
        type=str,
        default=None,
        help="Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch. æ˜¯å¦åº”è¯¥åœ¨æ¯næ­¥ç»“æŸæ—¶ä¿å­˜å„ç§çŠ¶æ€ æˆ–è€…åœ¨æ¯ä¸ª epoch æœ«å°¾ä¿å­˜ ",
    )
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="If the training should continue from a checkpoint folder. è®­ç»ƒæ˜¯å¦ä»æ£€æŸ¥ç‚¹ æ–‡ä»¶å¤¹ ç»§ç»­",
    )
    parser.add_argument(
        "--with_tracking",
        type=bool,
        default=False,
        help="Whether to enable experiment trackers for logging. æ˜¯å¦å¯ç”¨å®éªŒè·Ÿè¸ªå™¨è¿›è¡Œæ—¥å¿—è®°å½• ",
    )
    parser.add_argument(
        "--report_to",
        type=str,
        default="tensorboard",
        help=(
            'The integration to report the results and logs to. Supported platforms are `"tensorboard"`,'
            ' `"wandb"`, `"comet_ml"` and `"clearml"`. Use `"all"` (default) to report to all integrations. '
            "Only applicable when `--with_tracking` is passed."
            "è¦å°†ç»“æœå’Œæ—¥å¿—æŠ¥å‘Šåˆ°çš„é›†æˆã€‚"
            'æ”¯æŒçš„å¹³å°æœ‰ "tensorboard"ã€"wandb"ã€"comet_ml" å’Œ "clearml"ã€‚ä½¿ç”¨ "all"ï¼ˆé»˜è®¤ï¼‰å‘æ‰€æœ‰é›†æˆæŠ¥å‘Šã€‚'
            "åªæœ‰åœ¨ä¼ é€’ --with_tracking æ—¶æ‰é€‚ç”¨ã€‚"
        ),
    )
    parser.add_argument(
        "--ignore_mismatched_sizes",
        type=bool,
        default=True,
        help="Whether or not to enable to load a pretrained model whose head dimensions are different. æ˜¯å¦å¯ç”¨åŠ è½½å¤´éƒ¨ç»´åº¦ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ ",
    )
    parser.add_argument(
        "--low_cpu_mem_usage",
        type=bool,
        default=False,
        help=(
            "å½“ low_cpu_mem_usage=True æ—¶,å®ƒä¼šå…ˆåˆ›å»ºä¸€ä¸ªç©ºç™½çš„æ¨¡å‹å¯¹è±¡,åªæœ‰åœ¨å®é™…åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶æ‰ä¼šçœŸæ­£æ„é€ æ¨¡å‹å‚æ•°å¼ é‡ã€‚è¿™ç§å»¶è¿ŸåŠ è½½æ–¹å¼å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹åŠ è½½æ—¶çš„å†…å­˜å³°å€¼ä½¿ç”¨é‡ã€‚ "
            "éœ€è¦æ³¨æ„çš„æ˜¯,low_cpu_mem_usage ä¹Ÿä¼šå¸¦æ¥ä¸€äº›é¢å¤–çš„è®¡ç®—å¼€é”€,å› ä¸ºå®ƒéœ€è¦åˆ†åˆ«æ„é€ æ¯ä¸€å±‚çš„å‚æ•°å¼ é‡ã€‚ä½†å¯¹äºå†…å­˜ç´§å¼ çš„æƒ…å†µ,è¿™ç‚¹é¢å¤–å¼€é”€æ˜¯å¯ä»¥æ¥å—çš„ã€‚"
            "å¦ä¸€æ–¹é¢,å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„å†…å­˜,å¹¶ä¸”æ›´è¿½æ±‚åŠ è½½é€Ÿåº¦,é‚£ä¹ˆå°±å¯ä»¥å°† low_cpu_mem_usage è®¾ç½®ä¸º False,ä»¥è·å¾—æ›´å¿«çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½é€Ÿåº¦ã€‚"
        ),
    )
    args = parser.parse_args()

    # Sanity checks
    if args.train_file is None and args.validation_file is None:
        raise ValueError("Need either a task name or a training/validation file.")
    else:
        if args.train_file is not None:
            extension = args.train_file.split(".")[-1]
            assert extension in ["csv", "json", "jsonl"], "`train_file` should be a csv or a json file."
        if args.validation_file is not None:
            extension = args.validation_file.split(".")[-1]
            assert extension in ["csv", "json", "jsonl"], "`validation_file` should be a csv or a json file."

    return args

