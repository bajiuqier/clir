{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / 'Desktop' / 'clir' / 'data' / 'wikidata' / 'base_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 过滤 query 的 QID ---------------------\n",
    "QID_search_results_file = str(HOME_DIR / 'base_test1_qid.csv')\n",
    "QID_filtered_search_results_file = str(HOME_DIR / 'base_test1_qid_filtered.csv')\n",
    "\n",
    "def filter_qid(original_file: str, filtered_file: str):\n",
    "\n",
    "    QID_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # columns_to_keep = ['query', 'search_term', 'id', 'label', 'description']\n",
    "    columns_to_keep = ['query_id', 'query', 'qid']\n",
    "\n",
    "    # 只取 columns_to_keep 列数据，然后去除 id 列 为NaN的行数据\n",
    "    QID_filtered_df = QID_df[columns_to_keep].dropna(subset=['qid'])\n",
    "    # 去除重复项\n",
    "    QID_filtered_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # 保存文件\n",
    "    QID_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "filter_qid(QID_search_results_file, QID_filtered_search_results_file)\n",
    "\n",
    "# QID_df = pd.read_csv(QID_search_results_file, encoding='utf-8')\n",
    "\n",
    "# columns_to_keep = ['query_id', 'query', 'qid']\n",
    "# QID_filtered_df = QID_df[columns_to_keep]\n",
    "\n",
    "# ddd = str(HOME_DIR / 'base_test1_qid.csv')\n",
    "\n",
    "# QID_filtered_df.to_csv(ddd, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 删除 实体、属性英文信息为空的行数据 ---------------------\n",
    "def filter_item_info(original_file: str, filtered_file: str):\n",
    "\n",
    "    item_info_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "    # 删除'label_en', 'description_en' 为空的行数据\n",
    "    item_info_df = item_info_df.dropna(subset=['label_en', 'description_en'], how='any')\n",
    "\n",
    "    item_info_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "query_entity_info_file = str(HOME_DIR / 'base_test2_query_entity_info.csv')\n",
    "query_entity_filtered_info_file = str(HOME_DIR / 'base_test2_query_entity_filtered_info.csv')\n",
    "\n",
    "filter_item_info(query_entity_info_file, query_entity_filtered_info_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 过滤三元组 （实体-关系-实体) ---------------------\n",
    "def filter_triplet_id(original_file: str, filtered_file: str):\n",
    "    # 读取 CSV 文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # 删除含有任何 NaN 值的行\n",
    "    triplet_id_df = triplet_id_df.dropna()\n",
    "\n",
    "    # 确保 AdjItem 列中的值是字符串类型，并且填充 NaN 值\n",
    "    triplet_id_df['adj_item_qid'] = triplet_id_df['adj_item_qid'].astype(str)\n",
    "\n",
    "    # 使用正则表达式过滤符合条件的行 匹配以 \"Q\" 开头后跟数字的字符串\n",
    "    # na=False 确保 NaN 值不会引起错误。\n",
    "    triplet_id_filtered_df = triplet_id_df[triplet_id_df['adj_item_qid'].str.match(r'^Q\\d+$', na=False)]\n",
    "\n",
    "    # 删除 重复行\n",
    "    triplet_id_filtered_df = triplet_id_filtered_df.drop_duplicates(keep='first')\n",
    "\n",
    "    # 将结果保存到 CSV 文件\n",
    "    triplet_id_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'triplet_id_595.csv')\n",
    "triplet_id_filtered_file = str(HOME_DIR / 'triplet_id_595_filtered.csv')\n",
    "\n",
    "filter_triplet_id(triplet_id_file, triplet_id_filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 获取 triplet_id 片段 ---------------------\n",
    "# 对于每一个item的每一个属性对应的最多 n个 adj_item\n",
    "def get_triplet_id_fragment(original_file: str, filtered_file: str, n: int):\n",
    "    # 读取原始文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # 设置随机种子\n",
    "    seed = 33\n",
    "\n",
    "    # 创建一个空的列表来存储结果\n",
    "    result_list = []\n",
    "\n",
    "    # 对数据进行分组\n",
    "    grouped = triplet_id_df.groupby(['item_qid', 'property_qid'])\n",
    "\n",
    "    # 遍历每个分组\n",
    "    for (item_qid, property_qid), group in grouped:\n",
    "        # 如果组的大小小于或等于 n，直接添加整个组\n",
    "        if len(group) <= n:\n",
    "            result_list.append(group)\n",
    "        else:\n",
    "            # 否则，随机选择 n 个样本\n",
    "            sampled = group.sample(n, random_state=seed)\n",
    "            result_list.append(sampled)\n",
    "\n",
    "    # 将结果列表连接成一个 DataFrame\n",
    "    result_df = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "    # 将结果保存到新的 CSV 文件中\n",
    "    result_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'triplet_id_filtered.csv')\n",
    "triplet_id_fragment_file = str(HOME_DIR / 'triplet_id_fragment.csv')\n",
    "\n",
    "get_triplet_id_fragment(triplet_id_file, triplet_id_fragment_file, n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = str(Path.home() / 'Desktop' / 'clir' / 'models' / 'models--xlm-roberta-base')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='C:\\Users\\yanghe\\Desktop\\clir\\models\\models--xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
