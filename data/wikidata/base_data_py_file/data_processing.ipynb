{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / 'Desktop' / 'clir' / 'data' / 'wikidata' / 'base_data_file'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 过滤 query 的 QID ---------------------\n",
    "def filter_qid(original_file: str, filtered_file: str):\n",
    "\n",
    "    qid_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # columns_to_keep = ['query', 'search_term', 'id', 'label', 'description']\n",
    "    columns_to_keep = ['query_id', 'query_text', 'q_item_qid']\n",
    "\n",
    "    # 只取 columns_to_keep 列数据，然后去除 id 列 为NaN的行数据\n",
    "    qid_filtered_df = qid_df[columns_to_keep].dropna(subset=['q_item_qid'])\n",
    "    # 去除重复项\n",
    "    qid_filtered_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # 保存文件\n",
    "    qid_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "query_entity_qid_file = str(HOME_DIR / 'base_test1_qid.csv')\n",
    "query_entity_qid_filtered_file = str(HOME_DIR / 'base_test1_qid_filtered.csv')\n",
    "\n",
    "filter_qid(query_entity_qid_file, query_entity_qid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_test2_adj_item_info_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 删除 实体、属性英文信息为空的行数据 ---------------------\n",
    "def filter_item_info(original_file: str, filtered_file: str):\n",
    "\n",
    "    item_info_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "    # 删除 'label_en', 'description_en' 为空的行数据\n",
    "    item_info_df = item_info_df.dropna(subset=['label_en', 'description_en'], how='any')\n",
    "\n",
    "    # 删除重复数据\n",
    "    item_info_df.drop_duplicates(inplace=True)\n",
    "    item_info_df.drop_duplicates(subset=\"item_qid\", keep=\"first\", inplace=True)\n",
    "\n",
    "    item_info_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "item_info_file = str(HOME_DIR / 'base_test2_adj_item_info.csv')\n",
    "item_filtered_info_file = str(HOME_DIR / 'base_test2_adj_item_info_filtered.csv')\n",
    "\n",
    "filter_item_info(item_info_file, item_filtered_info_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_test11_triplet_id_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 过滤三元组 （实体-关系-实体) ---------------------\n",
    "def filter_triplet_id(original_file: str, filtered_file: str):\n",
    "    # 读取 CSV 文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8').astype(str)\n",
    "\n",
    "    # 删除含有任何 NaN 值的行\n",
    "    triplet_id_df = triplet_id_df.dropna()\n",
    "\n",
    "    # 使用正则表达式过滤符合条件的行 匹配以 \"Q\" 开头后跟数字的字符串\n",
    "    # na=False 确保 NaN 值不会引起错误。\n",
    "    triplet_id_filtered_df = triplet_id_df[triplet_id_df['adj_item_qid'].str.match(r'^Q\\d+$', na=False)]\n",
    "    triplet_id_filtered_df = triplet_id_filtered_df[triplet_id_filtered_df['property_qid'].str.match(r'^P\\d+$', na=False)]\n",
    "\n",
    "    # 删除 重复行\n",
    "    triplet_id_filtered_df = triplet_id_filtered_df.drop_duplicates(keep='first')\n",
    "\n",
    "    # 将结果保存到 CSV 文件\n",
    "    triplet_id_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'base_test11_triplet_id.csv')\n",
    "triplet_id_filtered_file = str(HOME_DIR / 'base_test11_triplet_id_filtered.csv')\n",
    "\n",
    "filter_triplet_id(triplet_id_file, triplet_id_filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 获取 triplet_id 片段 ---------------------\n",
    "# 对于每一个item的每一个属性对应的最多 n个 adj_item\n",
    "def get_triplet_id_fragment(original_file: str, filtered_file: str, n: int):\n",
    "    # 读取原始文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # 设置随机种子\n",
    "    seed = 33\n",
    "\n",
    "    # 创建一个空的列表来存储结果\n",
    "    result_list = []\n",
    "\n",
    "    # 对数据进行分组\n",
    "    grouped = triplet_id_df.groupby(['item_qid', 'property_qid'])\n",
    "\n",
    "    # 遍历每个分组\n",
    "    for (item_qid, property_qid), group in grouped:\n",
    "        # 如果组的大小小于或等于 n，直接添加整个组\n",
    "        if len(group) <= n:\n",
    "            result_list.append(group)\n",
    "        else:\n",
    "            # 否则，随机选择 n 个样本\n",
    "            sampled = group.sample(n, random_state=seed)\n",
    "            result_list.append(sampled)\n",
    "\n",
    "    # 将结果列表连接成一个 DataFrame\n",
    "    result_df = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "    # 将结果保存到新的 CSV 文件中\n",
    "    result_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'triplet_id_filtered.csv')\n",
    "triplet_id_fragment_file = str(HOME_DIR / 'triplet_id_fragment.csv')\n",
    "\n",
    "get_triplet_id_fragment(triplet_id_file, triplet_id_fragment_file, n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不存在空值\n"
     ]
    }
   ],
   "source": [
    "# 使用翻译引擎填充完缺失值后 检查是否还存在 空值 如果还存在空值 数量不多的情况下 手动 翻译 填充\n",
    "# 读取文件\n",
    "item_info_filled_file = str(HOME_DIR / 'base_test2_query_entity_info_filled.csv')\n",
    "item_info_filled_df = pd.read_csv(item_info_filled_file, encoding='utf-8')\n",
    "# 将存在空值的行的 index 转成列表\n",
    "empty_index_list=item_info_filled_df[item_info_filled_df.isnull().any(axis=1)].index.to_list()\n",
    "\n",
    "# \n",
    "if len(empty_index_list) == 0:\n",
    "    print(\"不存在空值\")\n",
    "else:\n",
    "    empty_qid_list = []\n",
    "    for index in empty_index_list:\n",
    "        item_qid = item_info_filled_df.loc[index]['item_qid']\n",
    "        empty_qid_list.append(item_qid)\n",
    "    print(\"存在空值的实体的 qid 列表：\")\n",
    "    print(f\"{empty_qid_list}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yandex 翻译 调用测试\n",
    "# import requests\n",
    "\n",
    "# text = \"Yu Fei\"\n",
    "\n",
    "# url = f\"https://translate.yandex.com/?source_lang=en&target_lang=kk&text={text}\" \n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# print(response.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成，输出文件: C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_adj_item_info\\base_adj_item_info.csv\n"
     ]
    }
   ],
   "source": [
    "from utils import merge_csv_files\n",
    "\n",
    "ADJ_ITEM_INFO_HOME_DIR = Path.home() / 'Desktop' / 'clir' / 'data' / 'wikidata' / 'base_adj_item_info'\n",
    "\n",
    "# 合并所有的query-entity信息\n",
    "pattern = r'base_adj_item_info_\\d+\\.csv'\n",
    "\n",
    "folder_path = str(ADJ_ITEM_INFO_HOME_DIR)\n",
    "output_file = str(ADJ_ITEM_INFO_HOME_DIR / 'base_adj_item_info.csv')\n",
    "\n",
    "merge_csv_files(folder_path=folder_path, output_file=output_file, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_test2_triplet_id_final.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 重新整理 triplet id ---------------------\n",
    "def rearranging_triplet_id(triplet_id_file: str, filter_reference_file: str, final_triplet_id_file: str):\n",
    "\n",
    "    triplet_id_df = pd.read_csv(triplet_id_file, encoding='utf-8')\n",
    "    filter_reference_df = pd.read_csv(filter_reference_file, encoding='utf-8')\n",
    "\n",
    "    adj_item_qids = set(filter_reference_df[\"item_qid\"])\n",
    "\n",
    "    final_triplet_id_df = triplet_id_df[triplet_id_df[\"adj_item_qid\"].isin(adj_item_qids)]\n",
    "    final_triplet_id_df.to_csv(final_triplet_id_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{final_triplet_id_file}\")\n",
    "\n",
    "triplet_id_file = HOME_DIR / 'base_test2_triplet_id_filtered.csv'\n",
    "filter_reference_file = HOME_DIR / 'base_test2_adj_item_info_filled.csv'\n",
    "final_triplet_id_file = HOME_DIR / 'base_test2_triplet_id_final.csv'\n",
    "\n",
    "rearranging_triplet_id(\n",
    "    triplet_id_file=triplet_id_file,\n",
    "    filter_reference_file=filter_reference_file,\n",
    "    final_triplet_id_file=final_triplet_id_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_test2_query_entity_qid_final.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 重新整理 query entity qid ---------------------\n",
    "def rearranging_query_entity_qid(query_entity_qid_file: str, filter_reference_file: str, final_query_entity_qid_file: str):\n",
    "\n",
    "    query_entity_qid_df = pd.read_csv(query_entity_qid_file, encoding='utf-8')\n",
    "    filter_reference_df = pd.read_csv(filter_reference_file, encoding='utf-8')\n",
    "\n",
    "    q_item_qids = set(filter_reference_df[\"item_qid\"])\n",
    "\n",
    "    final_query_entity_qid_df = query_entity_qid_df[query_entity_qid_df[\"q_item_qid\"].isin(q_item_qids)]\n",
    "    final_query_entity_qid_df.to_csv(final_query_entity_qid_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{final_query_entity_qid_file}\")\n",
    "\n",
    "query_entity_qid_file = HOME_DIR / 'base_test2_query_entity_qid_filtered.csv'\n",
    "filter_reference_file = HOME_DIR / 'base_test2_triplet_id_final.csv'\n",
    "final_query_entity_qid_file = HOME_DIR / 'base_test2_query_entity_qid_final.csv'\n",
    "\n",
    "rearranging_query_entity_qid(\n",
    "    query_entity_qid_file=query_entity_qid_file,\n",
    "    filter_reference_file=filter_reference_file,\n",
    "    final_query_entity_qid_file=final_query_entity_qid_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据合并完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data_file\\base_test_query_entity_qid_final.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并 test 相关文件\n",
    "\n",
    "# 合并 query_entity_qid 文件\n",
    "test1_query_entity_qid_file = str(HOME_DIR / \"base_test1_query_entity_qid_final.csv\")\n",
    "test2_query_entity_qid_file = str(HOME_DIR / \"base_test2_query_entity_qid_final.csv\")\n",
    "test_query_entity_qid_file = str(HOME_DIR / \"base_test_query_entity_qid_final.csv\")\n",
    "\n",
    "test1_query_entity_qid_df = pd.read_csv(test1_query_entity_qid_file, encoding='utf-8')\n",
    "test2_query_entity_qid_df = pd.read_csv(test2_query_entity_qid_file, encoding='utf-8')\n",
    "\n",
    "test_query_entity_qid_df = pd.concat([test1_query_entity_qid_df, test2_query_entity_qid_df], ignore_index=True)\n",
    "if test_query_entity_qid_df.duplicated(subset=\"query_id\").any():\n",
    "    raise ValueError(\"合并文件后 query_id 存在重复数据 请检查 并删除\")\n",
    "elif test_query_entity_qid_df.duplicated(subset=\"q_item_qid\").any():\n",
    "    raise ValueError(\"合并文件后 q_item_qid 存在重复数据 请检查 并删除\")\n",
    "else:\n",
    "    test_query_entity_qid_df.to_csv(test_query_entity_qid_file, index=False, encoding='utf-8')\n",
    "    print(f\"数据合并完成 文件存储在了{test_query_entity_qid_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据合并完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data_file\\base_test_triplet_id_final.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并 triplet_id 文件\n",
    "test1_triplet_id_file = str(HOME_DIR / \"base_test1_triplet_id_final.csv\")\n",
    "test2_triplet_id_file = str(HOME_DIR / \"base_test2_triplet_id_final.csv\")\n",
    "test_triplet_id_file = str(HOME_DIR / \"base_test_triplet_id_final.csv\")\n",
    "\n",
    "test1_triplet_id_df = pd.read_csv(test1_triplet_id_file, encoding='utf-8')\n",
    "test2_triplet_id_df = pd.read_csv(test2_triplet_id_file, encoding='utf-8')\n",
    "\n",
    "test_triplet_id_df = pd.concat([test1_triplet_id_df, test2_triplet_id_df], ignore_index=True)\n",
    "\n",
    "item_num = len(set(test1_triplet_id_df[\"item_qid\"])) + len(set(test2_triplet_id_df[\"item_qid\"]))\n",
    "item_merged_num = len(set(test_triplet_id_df[\"item_qid\"]))\n",
    "\n",
    "if item_num == item_merged_num:\n",
    "    test_triplet_id_df.to_csv(test_triplet_id_file, index=False, encoding='utf-8')\n",
    "    print(f\"数据合并完成 文件存储在了{test_triplet_id_file}\")\n",
    "else:\n",
    "    raise ValueError(\"合并文件后 存在重复 item_qid 数据 请检查 并删除\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据合并完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data_file\\base_test_query_entity_info_filled.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并 query_entity_info \n",
    "\n",
    "test1_query_entity_info_file = str(HOME_DIR / \"base_test1_query_entity_info_filled.csv\")\n",
    "test2_query_entity_info_file = str(HOME_DIR / \"base_test2_query_entity_info_filled.csv\")\n",
    "test_query_entity_info_file = str(HOME_DIR / \"base_test_query_entity_info_filled.csv\")\n",
    "\n",
    "test1_query_entity_info_df = pd.read_csv(test1_query_entity_info_file, encoding='utf-8')\n",
    "test2_query_entity_info_df = pd.read_csv(test2_query_entity_info_file, encoding='utf-8')\n",
    "\n",
    "test_query_entity_info_df = pd.concat([test1_query_entity_info_df, test2_query_entity_info_df], ignore_index=True)\n",
    "\n",
    "if test_query_entity_info_df.duplicated(subset=\"item_qid\").any():\n",
    "    raise ValueError(\"合并文件后 item_qid 存在重复数据 请检查 并删除\")\n",
    "else:\n",
    "    test_query_entity_info_df.to_csv(test_query_entity_info_file, index=False, encoding='utf-8')\n",
    "    print(f\"数据合并完成 文件存储在了{test_query_entity_info_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据合并完成 文件中 item_qid 存在重复项\n",
      "对重复项 进行仅保留一条数据 处理\n",
      "文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data_file\\base_test_adj_item_info_filled.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并 adj_item_info 文件\n",
    "test1_adj_item_info_file = str(HOME_DIR / \"base_test1_adj_item_info_filled.csv\")\n",
    "test2_adj_item_info_file = str(HOME_DIR / \"base_test2_adj_item_info_filled.csv\")\n",
    "test_adj_item_info_file = str(HOME_DIR / \"base_test_adj_item_info_filled.csv\")\n",
    "\n",
    "test1_adj_item_info_df = pd.read_csv(test1_adj_item_info_file, encoding='utf-8')\n",
    "test2_adj_item_info_df = pd.read_csv(test2_adj_item_info_file, encoding='utf-8')\n",
    "\n",
    "test_adj_item_info_df = pd.concat([test1_adj_item_info_df, test2_adj_item_info_df], ignore_index=True)\n",
    "\n",
    "if test_adj_item_info_df.duplicated(subset=\"item_qid\").any():\n",
    "    print(\"数据合并完成 文件中 item_qid 存在重复项\")\n",
    "    \n",
    "    test_adj_item_info_df.drop_duplicates(subset=\"item_qid\", keep=\"first\", inplace=True)\n",
    "    print(\"对重复项 进行仅保留一条数据 处理\")\n",
    "\n",
    "    test_adj_item_info_df.to_csv(test_adj_item_info_file, index=False, encoding='utf-8')\n",
    "    print(f\"文件存储在了{test_adj_item_info_file}\")\n",
    "\n",
    "else:\n",
    "    test_adj_item_info_df.to_csv(test_adj_item_info_file, index=False, encoding='utf-8')\n",
    "    print(f\"数据合并完成 文件存储在了{test_adj_item_info_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据合并完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data_file\\base_test_qrels.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并 test_qrels 文件\n",
    "test1_qrels_file = str(HOME_DIR / \"base_test1_qrels.csv\")\n",
    "test2_qrels_file = str(HOME_DIR / \"base_test2_qrels.csv\")\n",
    "test_qrels_file = str(HOME_DIR / \"base_test_qrels.csv\")\n",
    "\n",
    "test1_qrels_df = pd.read_csv(test1_qrels_file, encoding='utf-8')\n",
    "test2_qrels_df = pd.read_csv(test2_qrels_file, encoding='utf-8')\n",
    "\n",
    "test_qrels_df = pd.concat([test1_qrels_df, test2_qrels_df], ignore_index=True)\n",
    "\n",
    "query_id_num = len(set(test1_qrels_df[\"query_id\"])) + len(set(test2_qrels_df[\"query_id\"]))\n",
    "query_id_merged_num = len(set(test_qrels_df[\"query_id\"]))\n",
    "\n",
    "if query_id_num == query_id_merged_num:\n",
    "    test_qrels_df.to_csv(test_qrels_file, index=False, encoding='utf-8')\n",
    "    print(f\"数据合并完成 文件存储在了{test_qrels_file}\")\n",
    "else:\n",
    "    raise ValueError(\"合并文件后 存在重复 item_qid 数据 请检查 并删除\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
