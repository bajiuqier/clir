{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / 'Desktop' / 'clir' / 'data' / 'wikidata' / 'base_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 过滤 query 的 QID ---------------------\n",
    "def filter_qid(original_file: str, filtered_file: str):\n",
    "\n",
    "    QID_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # columns_to_keep = ['query', 'search_term', 'id', 'label', 'description']\n",
    "    columns_to_keep = ['query_id', 'query_text', 'q_item_qid']\n",
    "\n",
    "    # 只取 columns_to_keep 列数据，然后去除 id 列 为NaN的行数据\n",
    "    QID_filtered_df = QID_df[columns_to_keep].dropna(subset=['q_item_qid'])\n",
    "    # 去除重复项\n",
    "    QID_filtered_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # 保存文件\n",
    "    QID_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "query_entity_qid_file = str(HOME_DIR / 'base_test1_qid.csv')\n",
    "query_entity_qid_filtered_file = str(HOME_DIR / 'base_test1_qid_filtered.csv')\n",
    "\n",
    "filter_qid(query_entity_qid_file, query_entity_qid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 删除 实体、属性英文信息为空的行数据 ---------------------\n",
    "def filter_item_info(original_file: str, filtered_file: str):\n",
    "\n",
    "    item_info_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "    # 删除'label_en', 'description_en' 为空的行数据\n",
    "    item_info_df = item_info_df.dropna(subset=['label_en', 'description_en'], how='any')\n",
    "\n",
    "    # 删除重复数据\n",
    "    item_info_df.drop_duplicates(inplace=True)\n",
    "    item_info_df.drop_duplicates(subset=\"item_qid\", keep=\"first\", inplace=True)\n",
    "\n",
    "    item_info_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "item_info_file = str(HOME_DIR / 'base_train_adj_item_info.csv')\n",
    "item_filtered_info_file = str(HOME_DIR / 'base_train_adj_item_filtered_info.csv')\n",
    "\n",
    "filter_item_info(item_info_file, item_filtered_info_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_train_triplet_id_dddd.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 过滤三元组 （实体-关系-实体) ---------------------\n",
    "def filter_triplet_id(original_file: str, filtered_file: str):\n",
    "    # 读取 CSV 文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8').astype(str)\n",
    "\n",
    "    # 删除含有任何 NaN 值的行\n",
    "    triplet_id_df = triplet_id_df.dropna()\n",
    "\n",
    "    # 使用正则表达式过滤符合条件的行 匹配以 \"Q\" 开头后跟数字的字符串\n",
    "    # na=False 确保 NaN 值不会引起错误。\n",
    "    triplet_id_filtered_df = triplet_id_df[triplet_id_df['adj_item_qid'].str.match(r'^Q\\d+$', na=False)]\n",
    "    triplet_id_filtered_df = triplet_id_filtered_df[triplet_id_filtered_df['property_qid'].str.match(r'^P\\d+$', na=False)]\n",
    "\n",
    "    # 删除 重复行\n",
    "    triplet_id_filtered_df = triplet_id_filtered_df.drop_duplicates(keep='first')\n",
    "\n",
    "    # 将结果保存到 CSV 文件\n",
    "    triplet_id_filtered_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{filtered_file}\")\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'base_train_triplet_id.csv')\n",
    "triplet_id_filtered_file = str(HOME_DIR / 'base_train_triplet_id_dddd.csv')\n",
    "\n",
    "filter_triplet_id(triplet_id_file, triplet_id_filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 获取 triplet_id 片段 ---------------------\n",
    "# 对于每一个item的每一个属性对应的最多 n个 adj_item\n",
    "def get_triplet_id_fragment(original_file: str, filtered_file: str, n: int):\n",
    "    # 读取原始文件\n",
    "    triplet_id_df = pd.read_csv(original_file, encoding='utf-8')\n",
    "\n",
    "    # 设置随机种子\n",
    "    seed = 33\n",
    "\n",
    "    # 创建一个空的列表来存储结果\n",
    "    result_list = []\n",
    "\n",
    "    # 对数据进行分组\n",
    "    grouped = triplet_id_df.groupby(['item_qid', 'property_qid'])\n",
    "\n",
    "    # 遍历每个分组\n",
    "    for (item_qid, property_qid), group in grouped:\n",
    "        # 如果组的大小小于或等于 n，直接添加整个组\n",
    "        if len(group) <= n:\n",
    "            result_list.append(group)\n",
    "        else:\n",
    "            # 否则，随机选择 n 个样本\n",
    "            sampled = group.sample(n, random_state=seed)\n",
    "            result_list.append(sampled)\n",
    "\n",
    "    # 将结果列表连接成一个 DataFrame\n",
    "    result_df = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "    # 将结果保存到新的 CSV 文件中\n",
    "    result_df.to_csv(filtered_file, index=False, encoding='utf-8')\n",
    "\n",
    "triplet_id_file = str(HOME_DIR / 'triplet_id_filtered.csv')\n",
    "triplet_id_fragment_file = str(HOME_DIR / 'triplet_id_fragment.csv')\n",
    "\n",
    "get_triplet_id_fragment(triplet_id_file, triplet_id_fragment_file, n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不存在空值\n"
     ]
    }
   ],
   "source": [
    "# 使用翻译引擎填充完缺失值后 检查是否还存在 空值 如果还存在空值 数量不多的情况下 手动 翻译 填充\n",
    "# 读取文件\n",
    "item_info_filled_file = str(HOME_DIR / 'base_train_query_entity_filled_info.csv')\n",
    "item_info_filled_df = pd.read_csv(item_info_filled_file, encoding='utf-8')\n",
    "# 将存在空值的行的 index 转成列表\n",
    "empty_index_list=item_info_filled_df[item_info_filled_df.isnull().any(axis=1)].index.to_list()\n",
    "\n",
    "# \n",
    "if len(empty_index_list) == 0:\n",
    "    print(\"不存在空值\")\n",
    "else:\n",
    "    empty_qid_list = []\n",
    "    for index in empty_index_list:\n",
    "        item_qid = item_info_filled_df.loc[index]['item_qid']\n",
    "        empty_qid_list.append(item_qid)\n",
    "    print(\"存在空值的实体的 qid 列表：\")\n",
    "    print(f\"{empty_qid_list}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yandex 翻译 调用测试\n",
    "# import requests\n",
    "\n",
    "# text = \"Yu Fei\"\n",
    "\n",
    "# url = f\"https://translate.yandex.com/?source_lang=en&target_lang=kk&text={text}\" \n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# print(response.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成，输出文件: C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_train_adj_item_info\\base_train_adj_item_info.csv\n"
     ]
    }
   ],
   "source": [
    "from utils import merge_csv_files\n",
    "\n",
    "ADJ_ITEM_INFO_HOME_DIR = Path.home() / 'Desktop' / 'clir' / 'data' / 'wikidata' / 'base_train_adj_item_info'\n",
    "\n",
    "# 合并所有的query-entity信息\n",
    "pattern = r'base_train_adj_item_info_\\d+\\.csv'\n",
    "\n",
    "folder_path = str(ADJ_ITEM_INFO_HOME_DIR)\n",
    "output_file = str(ADJ_ITEM_INFO_HOME_DIR / 'base_train_adj_item_info.csv')\n",
    "\n",
    "merge_csv_files(folder_path=folder_path, output_file=output_file, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了C:\\Users\\bajiuqier\\Desktop\\clir\\data\\wikidata\\base_data\\base_train_triplet_id_fragment_3_final.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 重新整理 triplet id ---------------------\n",
    "def rearranging_triplet_id(triplet_id_file: str, filter_reference_file: str, final_triplet_id_file: str):\n",
    "\n",
    "    triplet_id_df = pd.read_csv(triplet_id_file, encoding='utf-8')\n",
    "    filter_reference_df = pd.read_csv(filter_reference_file, encoding='utf-8')\n",
    "\n",
    "    adj_item_qids = set(filter_reference_df[\"item_qid\"])\n",
    "\n",
    "    final_triplet_id_df = triplet_id_df[triplet_id_df[\"adj_item_qid\"].isin(adj_item_qids)]\n",
    "    final_triplet_id_df.to_csv(final_triplet_id_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{final_triplet_id_file}\")\n",
    "\n",
    "triplet_id_file = HOME_DIR / 'base_train_triplet_id_fragment_3.csv'\n",
    "filter_reference_file = HOME_DIR / 'base_train_adj_item_info_filled.csv'\n",
    "final_triplet_id_file = HOME_DIR / 'base_train_triplet_id_fragment_3_final.csv'\n",
    "\n",
    "rearranging_triplet_id(\n",
    "    triplet_id_file=triplet_id_file,\n",
    "    filter_reference_file=filter_reference_file,\n",
    "    final_triplet_id_file=final_triplet_id_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成 文件存储在了      query_id  query_text q_item_qid\n",
      "0         2625       7月19日      Q2726\n",
      "1          345          数论     Q12479\n",
      "2         4150          儒家      Q9581\n",
      "3          595        人口密度     Q22856\n",
      "4         2818     田纳西·威廉斯    Q134262\n",
      "...        ...         ...        ...\n",
      "7992   6839169  波奧亞 (夏威夷州)   Q3878252\n",
      "7994   6844149     魔進戰隊煌輝者  Q79818233\n",
      "7996   6845479       碳酸乙烯酯    Q421145\n",
      "7998   6852733      溫特施托克山   Q6981225\n",
      "8000   6861930      升变王车易位   Q2048586\n",
      "\n",
      "[5087 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 重新整理 query entity qid ---------------------\n",
    "def rearranging_query_entity_qid(query_entity_qid_file: str, filter_reference_file: str, final_query_entity_qid_file: str):\n",
    "\n",
    "    query_entity_qid_df = pd.read_csv(query_entity_qid_file, encoding='utf-8')\n",
    "    filter_reference_df = pd.read_csv(filter_reference_file, encoding='utf-8')\n",
    "\n",
    "    q_item_qids = set(filter_reference_df[\"item_qid\"])\n",
    "\n",
    "    final_query_entity_qid_df = query_entity_qid_df[query_entity_qid_df[\"q_item_qid\"].isin(q_item_qids)]\n",
    "    final_query_entity_qid_df.to_csv(final_query_entity_qid_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"数据处理完成 文件存储在了{final_query_entity_qid_file}\")\n",
    "\n",
    "query_entity_qid_file = HOME_DIR / 'base_train_query_entity_qid_filtered.csv'\n",
    "filter_reference_file = HOME_DIR / 'base_train_triplet_id_fragment_3_final.csv'\n",
    "final_query_entity_qid_file = HOME_DIR / 'base_train_query_entity_qid_final.csv'\n",
    "\n",
    "rearranging_query_entity_qid(\n",
    "    query_entity_qid_file=query_entity_qid_file,\n",
    "    filter_reference_file=filter_reference_file,\n",
    "    final_query_entity_qid_file=final_query_entity_qid_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
